# -*- coding: utf-8 -*-
"""TpFinalIIA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1goZpdijw7Bhd1PYtZ7UfYWWeUXnbMrl5
"""

import numpy as np
import pandas as pd

treino_path = 'nba_treino.csv'
teste_path = 'nba_teste.csv'

"""#Utils"""

def readAndStandardizeInput(file):
  df = pd.read_csv(file)
  # Standardization (mean 0 std 1)
  df.loc[:,'GP':'TOV'] = (df.loc[:,'GP':'TOV'] - df.loc[:,'GP':'TOV'].mean()) / df.loc[:,'GP':'TOV'].std()
  return df

def readInputFromFileList(files):
  dfs = []
  for file in files:
    df = pd.read_csv(file)
    dfs.append(df)
  df = pd.concat(dfs)
  df = df.reset_index(drop=True)

  return df

def _removeOutliers(df, nstds = 3):
  mean = df.loc[:,'GP':'TOV'].mean()
  std = df.loc[:,'GP':'TOV'].std()
  minLimit = mean - nstds*std
  maxLimit = mean + nstds*std
  maxOutlier = (df.loc[:,'GP':'TOV'] > maxLimit).sum(axis=1)
  minOutlier = (df.loc[:,'GP':'TOV'] < minLimit).sum(axis=1)
  outliers = maxOutlier + minOutlier
  return df[outliers < 1].copy()

def treatData(df, standardize = True, removeOutliers = True, ndesv = 3):
  df_copy = df.copy()
  if removeOutliers:
   df_copy = _removeOutliers(df_copy)

  if(standardize):
    # Standardization (mean 0 std 1)
    df_copy.loc[:,'GP':'TOV'] = (df_copy.loc[:,'GP':'TOV'] - df_copy.loc[:,'GP':'TOV'].mean()) / df_copy.loc[:,'GP':'TOV'].std()
  return df_copy

def calculateOneToManyEuclidianDistances(arr, arrs):
  return np.linalg.norm(arrs - arr, axis=1)

"""# KNN"""

def knnClassifier(train, instance, k):
  predicted_instance = instance.loc['GP':'TOV']
  distances = calculateOneToManyEuclidianDistances(predicted_instance, train.loc[:,'GP':'TOV'])
  sorting = np.argsort(distances)
  neighbors = train.loc[:,'TARGET_5Yrs'][sorting]
  kneighbors = neighbors[:k]
  if(kneighbors.mean() > 0.5):
    return 1
  else:
    return 0

def knnPredictMany(train, test, k):
  predicted = []
  for i in range(test.shape[0]):
    predicted.append(knnClassifier(train_df, test.loc[i,'GP':'TOV'], k))
  predicted = np.array(predicted)
  return predicted

from sklearn.neighbors import KNeighborsClassifier
def knnScikitPredictMany(train, test, k):
  neigh = KNeighborsClassifier(n_neighbors=k)
  neigh.fit(np.array(train.loc[:,'GP':'TOV']), np.array(train.loc[:,'TARGET_5Yrs']))
  return neigh.predict(np.array(test.loc[:,'GP':'TOV']))

def evaluate(pred, correct):
  TP = 0
  TN = 0
  FP = 0
  FN = 0
  for i in range(len(pred)):
    if(correct[i] == 1):
      if(pred[i] == 0):
        FN = FN + 1
      if(pred[i] == 1):
        TP = TP + 1
    if(correct[i] == 0):
      if(pred[i] == 0):
        TN = TN + 1
      if(pred[i] == 1):
        FP = FP + 1
  return (TP,TN,FP,FN)

def printMetricsAndConfusionMatrix(predicted, actual):
  (TP,TN,FP,FN) = evaluate(predicted, actual)
  data = {'1': [TP, FN],
        '0': [FP , TN]}
  confusion_matrix = pd.DataFrame(data, index=[1, 0])
  confusion_matrix.columns.name = 'Actual'
  confusion_matrix.index.name = 'Predicted'
  accuracy =  (TP + TN)/(TP + FP + TN + FN)
  recall = TP/(TP+TN)
  precision = TP/(TP+FP)
  f1 = (2*precision*recall)/(precision+recall)
  print(confusion_matrix)
  print("Accuracy -", accuracy)
  print("Recall -", recall)
  print("Precision -", precision)
  print("F1 -", f1)

print("KNN")

train_df = readAndStandardizeInput(treino_path)
test_df = readAndStandardizeInput(teste_path)
nTestInstances = test_df.shape[0]

actual = np.array(test_df.loc[:,'TARGET_5Yrs'])

kValues = [2, 10, 50, 80]

print("Implemententado")
for k in kValues:
  print('k -',k)
  predicted = knnPredictMany(train_df,test_df,k)
  printMetricsAndConfusionMatrix(predicted,actual)
  print()

print("__________________________________")
print("Scikit Learn")
for k in kValues:
  print('k -',k)
  predicted_scikit = knnScikitPredictMany(train_df,test_df,k)
  printMetricsAndConfusionMatrix(predicted_scikit,actual)
  print()

"""# K-Means

"""

print("________________________________________________________")
print("K-means")

def KMeansClassifyCluster(k, nrows, centroids, all_dataset_df_without_predicted):
  distances = np.zeros((k, nrows))
  for centroid_i, centroid in enumerate(centroids):
    distances[centroid_i,:] = calculateOneToManyEuclidianDistances(centroid, all_dataset_df_without_predicted)
  classification = np.argmin(distances, axis=0)
  return classification

def KMeansCluster(k,all_dataset_df):
  all_dataset_df_without_predicted = all_dataset_df.loc[:,'GP':'TOV'].copy()
  nrows = all_dataset_df_without_predicted.shape[0]
  centroids = rng.choice(all_dataset_df_without_predicted, size=k, replace=False)
  classification = None
  new_classification = KMeansClassifyCluster(k, nrows, centroids, all_dataset_df_without_predicted)

  while not(np.array_equal(classification, new_classification)):
    classification = new_classification
    for centroid_i in range(k):
      indexes = np.where(classification == centroid_i)[0]
      centroid = all_dataset_df_without_predicted.iloc[indexes].mean().values
      centroids[centroid_i,:] = centroid
    new_classification = KMeansClassifyCluster(k, nrows, centroids, all_dataset_df_without_predicted)
  return centroids

def getMetrics(TP, TN, FP, FN):
  accuracy =  (TP + TN)/(TP + FP + TN + FN)
  recall = TP/(TP+TN)
  precision = TP/(TP+FP)
  f1 = (2*precision*recall)/(precision+recall)
  return (accuracy, recall, precision, f1)

def evaluateKMeans(centroids, all_dataset_df, k):
  ans = all_dataset_df.loc[:,'TARGET_5Yrs']
  associated = np.zeros([k])
  total = all_dataset_df.shape[0]

  all_dataset_df_without_predicted = all_dataset_df.loc[:,'GP':'TOV'].copy()
  nrows = all_dataset_df.shape[0]
  classification = KMeansClassifyCluster(k, nrows, centroids, all_dataset_df_without_predicted)

  TP, TN, FP, FN = 0, 0, 0, 0
  print("count - most frequent - precision")
  correct = 0
  for centroid_i in range(k):
      indexes = np.where(classification == centroid_i)[0]
      associated[centroid_i] = np.round(ans.iloc[indexes].mean())
      current_total = indexes.shape[0]
      current_correct = (ans.iloc[indexes] == associated[centroid_i]).sum()
      correct = correct + current_correct
      print(indexes.shape[0], '-', associated[centroid_i], '-', current_correct/current_total)
      if(associated[centroid_i] == 1):
        TP = TP + current_correct
        FP = FP + current_total - current_correct
      else:
        TN = TN + current_correct
        FN = FN + current_total - current_correct
  print("Summary")

  (accuracy, recall, precision, f1) =getMetrics(TP, TN, FP, FN)
  print("Accuracy -", accuracy)
  print("Recall -", recall)
  print("Precision -", precision)
  print("F1 -", f1)

print("Implemententado")
all_dataset_df = readInputFromFileList([treino_path,teste_path])
k = 2
standardize = False
removeOutliers = True
ndesv = 3
df = treatData(all_dataset_df,standardize = standardize, removeOutliers = removeOutliers, ndesv = ndesv)
rng = np.random.default_rng(seed = 150)
centroids = KMeansCluster(k, df)
df_std = treatData(all_dataset_df,standardize = standardize, removeOutliers = False)
print("k -",k)
evaluateKMeans(centroids, df_std, k)

from sklearn.cluster import KMeans

print("__________________________________")
print("Scikit Learn")
all_dataset_df = readInputFromFileList([treino_path,teste_path])
k = 2
standardize = False
removeOutliers = True
ndesv = 3
df = treatData(all_dataset_df,standardize = standardize, removeOutliers = removeOutliers, ndesv = ndesv)
kmeans = KMeans(n_clusters=k, random_state=0).fit(df.loc[:,'GP':'TOV'])
scikit_centroids = kmeans.cluster_centers_
df_std = treatData(all_dataset_df,standardize = standardize, removeOutliers = False)
print("k -",k)

evaluateKMeans(scikit_centroids, df_std, k)

print("_________________________________________________")
print("distancias centroides")
print(np.linalg.norm(centroids[0] - scikit_centroids[1]))
print(np.linalg.norm(centroids[1] - scikit_centroids[0]))
print("_________________________________________________")

print("Implemententado")
all_dataset_df = readInputFromFileList([treino_path,teste_path])
k = 3
standardize = False
removeOutliers = True
ndesv = 3
df = treatData(all_dataset_df,standardize = standardize, removeOutliers = removeOutliers, ndesv = ndesv)
rng = np.random.default_rng(seed = 150)
centroids = KMeansCluster(k, df)
df_std = treatData(all_dataset_df,standardize = standardize, removeOutliers = False)
print("k -",k)
evaluateKMeans(centroids, df_std, k)

print("__________________________________")
print("Scikit Learn")
all_dataset_df = readInputFromFileList([treino_path,teste_path])
k = 3
standardize = False
removeOutliers = True
ndesv = 3
df = treatData(all_dataset_df,standardize = standardize, removeOutliers = removeOutliers, ndesv = ndesv)
kmeans = KMeans(n_clusters=k, random_state=0).fit(df.loc[:,'GP':'TOV'])
scikit_centroids = kmeans.cluster_centers_
df_std = treatData(all_dataset_df,standardize = standardize, removeOutliers = False)
print("k -",k)
evaluateKMeans(scikit_centroids, df_std, k)

print("_________________________________________________")
print("distancias centroides")
print(np.linalg.norm(centroids[0] - scikit_centroids[0]))
print(np.linalg.norm(centroids[1] - scikit_centroids[1]))
print(np.linalg.norm(centroids[2] - scikit_centroids[2]))
print("_________________________________________________")